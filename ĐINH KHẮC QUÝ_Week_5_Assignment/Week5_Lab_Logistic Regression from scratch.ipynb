{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AbxAt696ms58"
   },
   "source": [
    "# Lab - Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YkoZMtDfmyO7"
   },
   "source": [
    "## Logistic Regression from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 991,
     "status": "ok",
     "timestamp": 1596177799371,
     "user": {
      "displayName": "Minh Do",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhnQ6aJ2YnaevyJzhii-qGws6Y17w-cwWLqF5iP=s64",
      "userId": "12822549848477954436"
     },
     "user_tz": -420
    },
    "id": "AaFlqj51nbGm",
    "outputId": "0edf32f7-b03b-48df-ac56-81d6e80ea56e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hEGtW99Am75l"
   },
   "source": [
    "Dataset **Titanic**\n",
    "\n",
    "![alt text](https://upload.wikimedia.org/wikipedia/commons/thumb/d/db/Titanic-Cobh-Harbour-1912.JPG/330px-Titanic-Cobh-Harbour-1912.JPG)\n",
    "\n",
    "The dataset that we are working on is a list of passenger on the famous ship Titanic. On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing a lot passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n",
    "\n",
    "**Data dictionary**\n",
    " \n",
    "| Variable | Definition | Key |\n",
    "|:--:|:--:|:--:|\n",
    "| survival | Survival | 0 = No, 1 = Yes |\n",
    "| pclass | Ticket class, a proxy for socio-economic status (SES) | 1 = 1st, 2 = 2nd, 3 = 3rd |\n",
    "| sex | Gender | |\n",
    "| Age | Age in years | |\n",
    "| sibsp | # of siblings(brother,sister)/spouses(husband, wife) aboard the Titanic |\n",
    "| parch | # of parents/children aboard the Titanic. Some children travelled only with a nanny, therefore parch=0 for them |\n",
    "| ticket | Ticket number | |\n",
    "| fare | Passenger fare | |\n",
    "| cabin | Cabin number | |\n",
    "| embarked | Port of Embarkation | C=Cherbourg, Q=Queenstown, S=Southampton |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1519,
     "status": "ok",
     "timestamp": 1596177799904,
     "user": {
      "displayName": "Minh Do",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhnQ6aJ2YnaevyJzhii-qGws6Y17w-cwWLqF5iP=s64",
      "userId": "12822549848477954436"
     },
     "user_tz": -420
    },
    "id": "6l2ZsmISOlGC",
    "outputId": "da85cceb-ef0f-43f2-dc45-97b857a5b95a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>FamilySize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass  Sex   Age  FamilySize\n",
       "0         0       3    0  22.0           2\n",
       "1         1       1    1  38.0           2\n",
       "2         1       3    1  26.0           1\n",
       "3         1       1    1  35.0           2\n",
       "4         0       3    0  35.0           1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic = pd.read_csv('https://raw.githubusercontent.com/dhminh1024/practice_datasets/master/titanic.csv')\n",
    "\n",
    "# Data manipulation\n",
    "titanic.fillna(titanic['Age'].mean(), inplace=True)\n",
    "titanic.replace({'Sex':{'male':0, 'female':1}}, inplace=True)\n",
    "titanic['FamilySize'] = titanic['SibSp'] + titanic['Parch'] + 1\n",
    "titanic.drop(columns=['PassengerId', 'Name', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'], inplace=True)\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1518,
     "status": "ok",
     "timestamp": 1596177799905,
     "user": {
      "displayName": "Minh Do",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhnQ6aJ2YnaevyJzhii-qGws6Y17w-cwWLqF5iP=s64",
      "userId": "12822549848477954436"
     },
     "user_tz": -420
    },
    "id": "vjGt2y_-pPHz",
    "outputId": "76392e7d-04f5-4dcb-cb39-6e9323adc8da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (712, 4) (712, 1)\n",
      "Test set: (179, 4) (179, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = titanic[['Pclass', 'Sex', 'Age', 'FamilySize']].values\n",
    "y = titanic[['Survived']].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=102)\n",
    "\n",
    "print('Training set:', X_train.shape, y_train.shape)\n",
    "print('Test set:', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S79QyfXSqeXj"
   },
   "source": [
    "### Scikit-learn Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1928,
     "status": "ok",
     "timestamp": 1596177800318,
     "user": {
      "displayName": "Minh Do",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhnQ6aJ2YnaevyJzhii-qGws6Y17w-cwWLqF5iP=s64",
      "userId": "12822549848477954436"
     },
     "user_tz": -420
    },
    "id": "Ta6jRuopqPVV",
    "outputId": "63ea658a-131b-4630-ca87-04eb80be85ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.793296\n",
      "Confusion Matrix:\n",
      "[[97 17]\n",
      " [20 45]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.85      0.84       114\n",
      "           1       0.73      0.69      0.71        65\n",
      "\n",
      "    accuracy                           0.79       179\n",
      "   macro avg       0.78      0.77      0.77       179\n",
      "weighted avg       0.79      0.79      0.79       179\n",
      "\n",
      "Log loss: 0.039884782615024775\n",
      "w =  [[-1.18387774  2.56284417 -0.04074789 -0.21591208]]\n",
      "b =  [2.84100084]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# Create Logistics Regression model from X and y\n",
    "lg = LogisticRegression()\n",
    "lg.fit(X_train, y_train)\n",
    "predictions = lg.predict(X_test)\n",
    "\n",
    "# Show metrics\n",
    "print(\"Accuracy score: %f\" % accuracy_score(y_test, predictions))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "print('Log loss:', log_loss(y_test, predictions)/len(y_test))\n",
    "\n",
    "# Show parameters\n",
    "print('w = ', lg.coef_)\n",
    "print('b = ', lg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "keQAhakRrJWs"
   },
   "source": [
    "### Handmade Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IC0ZhoR3rXbO"
   },
   "source": [
    "**Forward Propagation:**\n",
    "$$Z = Xw + b$$\n",
    "$$\\hat{y} = \\sigma(Z) =\\sigma(Xw + b) $$\n",
    "$$J(w, b) = -\\frac{1}{m}\\sum_{i=1}^m{ \\Big( y^{(i)} log( \\hat{y}^{(i)}) + (1-y^{(i)}) log(1 - \\hat{y}^{(i)}) \\Big)} \\tag{5}$$\n",
    "\n",
    "**and Backward**\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X^T(\\hat{y}-y)\\tag{6}$$\n",
    "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (\\hat{y}^{(i)}-y^{(i)})\\tag{7}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1927,
     "status": "ok",
     "timestamp": 1596177800319,
     "user": {
      "displayName": "Minh Do",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhnQ6aJ2YnaevyJzhii-qGws6Y17w-cwWLqF5iP=s64",
      "userId": "12822549848477954436"
     },
     "user_tz": -420
    },
    "id": "adHOPx63q5xI"
   },
   "outputs": [],
   "source": [
    "# Initialize params\n",
    "def initialize_params(X):\n",
    "    '''Initialize w, b with zeros and return'''\n",
    "    # Your code here\n",
    "    w = np.zeros((X.shape[1], 1))\n",
    "    b = np.zeros((1, 1))\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1926,
     "status": "ok",
     "timestamp": 1596177800320,
     "user": {
      "displayName": "Minh Do",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhnQ6aJ2YnaevyJzhii-qGws6Y17w-cwWLqF5iP=s64",
      "userId": "12822549848477954436"
     },
     "user_tz": -420
    },
    "id": "LgabqgGhr8B5"
   },
   "outputs": [],
   "source": [
    "# Implement sigmoid\n",
    "def sigmoid(Z):\n",
    "    # Your code here\n",
    "    return 1/(1 + np.exp(-Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1923,
     "status": "ok",
     "timestamp": 1596177800320,
     "user": {
      "displayName": "Minh Do",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhnQ6aJ2YnaevyJzhii-qGws6Y17w-cwWLqF5iP=s64",
      "userId": "12822549848477954436"
     },
     "user_tz": -420
    },
    "id": "zK_0xR_fsFaD"
   },
   "outputs": [],
   "source": [
    "# Forward propagation\n",
    "def forward(w, b, X):\n",
    "    '''Return y_hat'''\n",
    "    # Your code here\n",
    "    Z = np.dot(X, w) + b\n",
    "    y_hat = sigmoid(Z) #activation function\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1922,
     "status": "ok",
     "timestamp": 1596177800321,
     "user": {
      "displayName": "Minh Do",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhnQ6aJ2YnaevyJzhii-qGws6Y17w-cwWLqF5iP=s64",
      "userId": "12822549848477954436"
     },
     "user_tz": -420
    },
    "id": "kCzZpPg9sX4c"
   },
   "outputs": [],
   "source": [
    "# Binary cross entropy loss\n",
    "def binary_cross_entropy(y, y_hat):\n",
    "    '''Calculate loss function J and return'''\n",
    "    # Your code here\n",
    "    J = -np.mean(y*np.log(y_hat) + (1-y)*np.log(1-y_hat)) #(n_samples, 1)\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1920,
     "status": "ok",
     "timestamp": 1596177800321,
     "user": {
      "displayName": "Minh Do",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhnQ6aJ2YnaevyJzhii-qGws6Y17w-cwWLqF5iP=s64",
      "userId": "12822549848477954436"
     },
     "user_tz": -420
    },
    "id": "MhuGl5ATtT2d"
   },
   "outputs": [],
   "source": [
    "# Backward propagation\n",
    "def backward(X, y, y_hat, w, b):\n",
    "    '''Calculate dw, db and return'''\n",
    "    # Your code here\n",
    "    m = X.shape[0]\n",
    "    dw = (1/m) * np.dot(X.T, (y_hat - y))\n",
    "    db = (1/m) * np.sum(y_hat - y, keepdims=True)\n",
    "    return dw, db\n",
    "\n",
    "# Update parameters\n",
    "def update_params(w, b, dw, db, learning_rate):\n",
    "    '''Update w, b and return'''\n",
    "    # Your code here\n",
    "    w = w - learning_rate * dw\n",
    "    b = b - learning_rate * db\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1919,
     "status": "ok",
     "timestamp": 1596177800322,
     "user": {
      "displayName": "Minh Do",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhnQ6aJ2YnaevyJzhii-qGws6Y17w-cwWLqF5iP=s64",
      "userId": "12822549848477954436"
     },
     "user_tz": -420
    },
    "id": "8J3nYXjMupQa"
   },
   "outputs": [],
   "source": [
    "# Training process\n",
    "def train(X, y, iterations, learning_rate):\n",
    "    '''Train w, b and return'''\n",
    "    # Your code here\n",
    "    w, b = initialize_params(X)\n",
    "\n",
    "    history = {'train_loss': np.array([]),\n",
    "               'test_loss': np.array([]),\n",
    "               'lr': np.array([])}\n",
    "\n",
    "    for i in range(iterations):\n",
    "        y_hat = forward(w, b, X)\n",
    "        J = binary_cross_entropy(y, y_hat)\n",
    "        history['train_loss'] = np.append(history['train_loss'], J)\n",
    "        J_test = binary_cross_entropy(y_test, forward(w, b, X_test))\n",
    "        history['test_loss'] = np.append(history['test_loss'], J_test)\n",
    "        if i % 100 == 0:\n",
    "            print(f'Step {i}: train_loss = {J}')\n",
    "        dw, db = backward(X, y, y_hat, w, b)\n",
    "        w, b = update_params(w, b, dw, db, learning_rate)\n",
    "    return w, b, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1918,
     "status": "ok",
     "timestamp": 1596177800323,
     "user": {
      "displayName": "Minh Do",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhnQ6aJ2YnaevyJzhii-qGws6Y17w-cwWLqF5iP=s64",
      "userId": "12822549848477954436"
     },
     "user_tz": -420
    },
    "id": "N6NGSUVpuVNA"
   },
   "outputs": [],
   "source": [
    "# Predict\n",
    "def predict(w, b, X):\n",
    "    '''Return predicted y of X'''\n",
    "    y_hat = forward(w, b, X)\n",
    "    return y_hat > 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JTUDeY_fwIXx"
   },
   "source": [
    "**Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1915,
     "status": "ok",
     "timestamp": 1596177800323,
     "user": {
      "displayName": "Minh Do",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhnQ6aJ2YnaevyJzhii-qGws6Y17w-cwWLqF5iP=s64",
      "userId": "12822549848477954436"
     },
     "user_tz": -420
    },
    "id": "mDgSepfDwEHy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: train_loss = 0.6931471805599453\n",
      "Step 100: train_loss = 0.6468251630263748\n",
      "Step 200: train_loss = 0.6326757129358918\n",
      "Step 300: train_loss = 0.620992935723151\n",
      "Step 400: train_loss = 0.6109658510132288\n",
      "Step 500: train_loss = 0.602152147668363\n",
      "Step 600: train_loss = 0.5942744614199724\n",
      "Step 700: train_loss = 0.5871431335121196\n",
      "Step 800: train_loss = 0.5806220436874141\n",
      "Step 900: train_loss = 0.5746106860323436\n",
      "Step 1000: train_loss = 0.5690333588537603\n",
      "Step 1100: train_loss = 0.5638320514323742\n",
      "Step 1200: train_loss = 0.5589615449002256\n",
      "Step 1300: train_loss = 0.554385962345795\n",
      "Step 1400: train_loss = 0.550076312149218\n",
      "Step 1500: train_loss = 0.5460087270411388\n",
      "Step 1600: train_loss = 0.5421631955903263\n",
      "Step 1700: train_loss = 0.5385226441635325\n",
      "Step 1800: train_loss = 0.535072269221707\n",
      "Step 1900: train_loss = 0.5317990489438789\n",
      "Step 2000: train_loss = 0.5286913836584277\n",
      "Step 2100: train_loss = 0.5257388290480368\n",
      "Step 2200: train_loss = 0.5229318963714717\n",
      "Step 2300: train_loss = 0.5202619012545813\n",
      "Step 2400: train_loss = 0.5177208478130647\n",
      "Step 2500: train_loss = 0.5153013385908951\n",
      "Step 2600: train_loss = 0.512996503461038\n",
      "Step 2700: train_loss = 0.5107999425434852\n",
      "Step 2800: train_loss = 0.5087056795653051\n",
      "Step 2900: train_loss = 0.5067081230717212\n",
      "Step 3000: train_loss = 0.5048020336054064\n",
      "Step 3100: train_loss = 0.502982495481241\n",
      "Step 3200: train_loss = 0.5012448921515325\n",
      "Step 3300: train_loss = 0.4995848844221315\n",
      "Step 3400: train_loss = 0.4979983909716992\n",
      "Step 3500: train_loss = 0.4964815707652122\n",
      "Step 3600: train_loss = 0.4950308070534441\n",
      "Step 3700: train_loss = 0.49364269272331085\n",
      "Step 3800: train_loss = 0.49231401681729636\n",
      "Step 3900: train_loss = 0.49104175207919104\n",
      "Step 4000: train_loss = 0.4898230434120959\n",
      "Step 4100: train_loss = 0.488655197155899\n",
      "Step 4200: train_loss = 0.48753567110731816\n",
      "Step 4300: train_loss = 0.4864620652175899\n",
      "Step 4400: train_loss = 0.48543211291204735\n",
      "Step 4500: train_loss = 0.48444367298294144\n",
      "Step 4600: train_loss = 0.48349472201248067\n",
      "Step 4700: train_loss = 0.4825833472875696\n",
      "Step 4800: train_loss = 0.48170774017143897\n",
      "Step 4900: train_loss = 0.48086618990045576\n",
      "Step 5000: train_loss = 0.4800570777770414\n",
      "Step 5100: train_loss = 0.4792788717319244\n",
      "Step 5200: train_loss = 0.4785301212309691\n",
      "Step 5300: train_loss = 0.47780945250363\n",
      "Step 5400: train_loss = 0.4771155640717108\n",
      "Step 5500: train_loss = 0.47644722255859384\n",
      "Step 5600: train_loss = 0.4758032587604641\n",
      "Step 5700: train_loss = 0.47518256396231456\n",
      "Step 5800: train_loss = 0.4745840864826773\n",
      "Step 5900: train_loss = 0.4740068284321108\n",
      "Step 6000: train_loss = 0.4734498426714737\n",
      "Step 6100: train_loss = 0.47291222995695653\n",
      "Step 6200: train_loss = 0.47239313625971086\n",
      "Step 6300: train_loss = 0.4718917502487367\n",
      "Step 6400: train_loss = 0.4714073009264412\n",
      "Step 6500: train_loss = 0.4709390554069974\n",
      "Step 6600: train_loss = 0.47048631682828956\n",
      "Step 6700: train_loss = 0.4700484223888518\n",
      "Step 6800: train_loss = 0.46962474150177974\n",
      "Step 6900: train_loss = 0.46921467405813627\n",
      "Step 7000: train_loss = 0.4688176487928684\n",
      "Step 7100: train_loss = 0.4684331217467234\n",
      "Step 7200: train_loss = 0.46806057481808544\n",
      "Step 7300: train_loss = 0.4676995143990601\n",
      "Step 7400: train_loss = 0.46734947009051225\n",
      "Step 7500: train_loss = 0.4670099934911138\n",
      "Step 7600: train_loss = 0.46668065705578793\n",
      "Step 7700: train_loss = 0.46636105301923964\n",
      "Step 7800: train_loss = 0.4660507923805488\n",
      "Step 7900: train_loss = 0.46574950394506553\n",
      "Step 8000: train_loss = 0.46545683342009586\n",
      "Step 8100: train_loss = 0.46517244256109513\n",
      "Step 8200: train_loss = 0.4648960083653009\n",
      "Step 8300: train_loss = 0.46462722230993586\n",
      "Step 8400: train_loss = 0.4643657896322994\n",
      "Step 8500: train_loss = 0.46411142864923743\n",
      "Step 8600: train_loss = 0.46386387011364244\n",
      "Step 8700: train_loss = 0.4636228566057874\n",
      "Step 8800: train_loss = 0.46338814195743555\n",
      "Step 8900: train_loss = 0.46315949070680024\n",
      "Step 9000: train_loss = 0.46293667758255047\n",
      "Step 9100: train_loss = 0.4627194870151721\n",
      "Step 9200: train_loss = 0.46250771267410073\n",
      "Step 9300: train_loss = 0.4623011570291399\n",
      "Step 9400: train_loss = 0.4620996309347744\n",
      "Step 9500: train_loss = 0.46190295323606984\n",
      "Step 9600: train_loss = 0.4617109503949353\n",
      "Step 9700: train_loss = 0.461523456135597\n",
      "Step 9800: train_loss = 0.4613403111082044\n",
      "Step 9900: train_loss = 0.4611613625695541\n"
     ]
    }
   ],
   "source": [
    "# Train the model and predict X_test\n",
    "w, b, history = train(X_train, y_train, iterations=10000, learning_rate=5e-3)\n",
    "predictions = predict(w, b, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5yXlcgeRwZ3v"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8156424581005587\n",
      "confusion matrix:\n",
      " [[101  13]\n",
      " [ 20  45]]\n",
      "classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.89      0.86       114\n",
      "           1       0.78      0.69      0.73        65\n",
      "\n",
      "    accuracy                           0.82       179\n",
      "   macro avg       0.81      0.79      0.80       179\n",
      "weighted avg       0.81      0.82      0.81       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "# Your code here\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "# Now apply those above metrics to evaluate your model\n",
    "# Your code here\n",
    "predictions = predict(w, b, X_test)\n",
    "print('accuracy:',accuracy_score(y_test,predictions))\n",
    "print('confusion matrix:\\n',confusion_matrix(y_test,predictions))\n",
    "print('classification report:\\n',classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1911,
     "status": "ok",
     "timestamp": 1596177800324,
     "user": {
      "displayName": "Minh Do",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhnQ6aJ2YnaevyJzhii-qGws6Y17w-cwWLqF5iP=s64",
      "userId": "12822549848477954436"
     },
     "user_tz": -420
    },
    "id": "Sd03Qlx8woet"
   },
   "outputs": [],
   "source": [
    "# Output of sklearn.LogisticRegression\n",
    "# Accuracy score: 0.793296\n",
    "# Confusion Matrix:\n",
    "# [[97 17]\n",
    "#  [20 45]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.83      0.85      0.84       114\n",
    "#            1       0.73      0.69      0.71        65\n",
    "\n",
    "#     accuracy                           0.79       179\n",
    "#    macro avg       0.78      0.77      0.77       179\n",
    "# weighted avg       0.79      0.79      0.79       179\n",
    "\n",
    "# Log loss: 0.039884782615024775\n",
    "# w =  [[-1.18387774  2.56284417 -0.04074789 -0.21591208]]\n",
    "# b =  [2.84100084]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UIj9GKy8Y9Pp"
   },
   "source": [
    "**Well done!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Week5_Lab_Logistic Regression from scratch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
